# Workshop Kubernetes/Faclo

https://git.sue.nl/maurice.commandeur/workshop_46_2022

Wat ik in deze workshop wil laten zien is hoe je met Faclo slecht gedragende
pods kunt opsporen.
Daarna gaan we de melding die Faclo geeft onderdrukken door een configuratie verandering.

# _Note to self_ Mijn setup veranderen voor minikube
Ik gebruik split-dns. dwz Dat ik op mijn laptop een dns server heb daaien die
dns requests van de klant naar de klant dns doe en andere dns request naar
specifieke andere dns servers.
`sudo brew services stop dnsmasq`
En zorg dat de dns entries vanuit de dhcp server actief worden.

# Setup omgeving
De installatie en configuratie is volgens [Falco in minikube](https://falco.org/docs/getting-started/third-party/learning/)

Tijdelijke directory maken waarin je bestanden staan.
```
mkdir -p ~/workshop-46-2022
mkdir -p ~/.minikube/files/etc/ssl/certs
cd ~/workshop-46-2022
```

Controle of minikube werkt en Falco met syscall source
```
minikube start
kubectl get pods --all-namespaces
helm repo add falcosecurity https://falcosecurity.github.io/charts
helm repo update
helm install falco --set driver.kind=ebpf --set tty=true falcosecurity/falco
```
Controleren op output van Falco
```
kubectl logs -l app.kubernetes.io/name=falco --all-containers
```


## bronnen
Je kunt meerdere bronnen confiureren waar Falco naar moet kijken.
We zetten de audit log in minikube aan

```
cat << EOF > ~/.minikube/files/etc/ssl/certs/audit-policy.yaml
apiVersion: audit.k8s.io/v1 # This is required.
kind: Policy
# Don't generate audit events for all requests in RequestReceived stage.
omitStages:
  -  "RequestReceived"
rules:
  # Log pod changes at RequestResponse level
  - level: RequestResponse
    resources:
    - group: ""
      # Resource "pods" doesn't match requests to any subresource of pods,
      # which is consistent with the RBAC policy.
      resources: ["pods", "deployments"]

  - level: RequestResponse
    resources:
    - group: "rbac.authorization.k8s.io"
      # Resource "pods" doesn't match requests to any subresource of pods,
      # which is consistent with the RBAC policy.
      resources: ["clusterroles", "clusterrolebindings"]

  # Log "pods/log", "pods/status" at Metadata level
  - level: Metadata
    resources:
    - group: ""
      resources: ["pods/log", "pods/status"]

  # Don't log requests to a configmap called "controller-leader"
  - level: None
    resources:
    - group: ""
      resources: ["configmaps"]
      resourceNames: ["controller-leader"]

  # Don't log watch requests by the "system:kube-proxy" on endpoints or services
  - level: None
    users: ["system:kube-proxy"]
    verbs: ["watch"]
    resources:
    - group: "" # core API group
      resources: ["endpoints", "services"]

  # Don't log authenticated requests to certain non-resource URL paths.
  - level: None
    userGroups: ["system:authenticated"]
    nonResourceURLs:
    - "/api*" # Wildcard matching.
    - "/version"

  # Log the request body of configmap changes in kube-system.
  - level: Request
    resources:
    - group: "" # core API group
      resources: ["configmaps"]
    # This rule only applies to resources in the "kube-system" namespace.
    # The empty string "" can be used to select non-namespaced resources.
    namespaces: ["kube-system"]

  # Log configmap changes in all other namespaces at the RequestResponse level.
  - level: RequestResponse
    resources:
    - group: "" # core API group
      resources: ["configmaps"]

  # Log secret changes in all other namespaces at the Metadata level.
  - level: Metadata
    resources:
    - group: "" # core API group
      resources: ["secrets"]

  # Log all other resources in core and extensions at the Request level.
  - level: Request
    resources:
    - group: "" # core API group
    - group: "extensions" # Version of group should NOT be included.

  # A catch-all rule to log all other requests at the Metadata level.
  - level: Metadata
    # Long-running requests like watches that fall under this rule will not
    # generate an audit event in RequestReceived.
    omitStages:
      - "RequestReceived"
EOF
```

En het bestand webhook-config.yaml
> A WebHook is an HTTP callback: an HTTP POST that occurs when something
> happens; a simple event-notification via HTTP POST. A web application
> implementing WebHooks will POST a message to a URL when certain things happen.
```
cat << EOF > ~/.minikube/files/etc/ssl/certs/webhook-config.yaml
apiVersion: v1
kind: Config
clusters:
- name: falco
  cluster:
    # certificate-authority: /path/to/ca.crt # for https
    server: http://localhost:30007/k8s-audit
contexts:
- context:
    cluster: falco
    user: ""
  name: default-context
current-context: default-context
preferences: {}
users: []
EOF
```
Daarna minikube stoppen en weer starten
```
minikube stop && \
minikube start \
    --extra-config=apiserver.audit-policy-file=/etc/ssl/certs/audit-policy.yaml \
    --extra-config=apiserver.audit-log-path=- \
    --extra-config=apiserver.audit-webhook-config-file=/etc/ssl/certs/webhook-config.yaml \
    --extra-config=apiserver.audit-webhook-batch-max-size=10 \
    --extra-config=apiserver.audit-webhook-batch-max-wait=5s \
    --cpus=4
```

Nu nog zorgen dat we de syscall en k8saudit bronnen gebruiken
```
cat << EOF > ~/workshop-46-2022/values-falco-syscall-k8saudit.yaml
driver:
  enabled: true

collectors:
  enabled: true

controller:
  kind: daemonset

services:
  - name: k8saudit-webhook
    type: NodePort
    ports:
      - port: 9765 # See plugin open_params
        nodePort: 30007
        protocol: TCP

tty: true

falco:
  rules_file:
    - /etc/falco/k8s_audit_rules.yaml
    - /etc/falco/rules.d
    - /etc/falco/falco_rules.yaml
  plugins:
    - name: k8saudit
      library_path: libk8saudit.so
      init_config:
        ""
        # maxEventBytes: 1048576
        # sslCertificate: /etc/falco/falco.pem
      open_params: "http://:9765/k8s-audit"
    - name: json
      library_path: libjson.so
      init_config: ""
  load_plugins: [k8saudit, json]
EOF
```

## Falco installeren
We installeren Falco via een helm chart
```
helm repo add falcosecurity https://falcosecurity.github.io/charts
helm repo update
helm install falco --set driver.kind=ebpf --values=~/workshop-46-2022/values-falco-syscall-k8saudit.yaml --set tty=true falcosecurity/falco
# Let op helm resolved dit pad (~) niet altijd goed
kubectl -n default get pods
kubectl logs -l app.kubernetes.io/name=falco --all-containers
```
Dan is het belangrijk dat je het volgende ziet in de log
Dit bewijst dat de ks8audit gelezen wordt.
```
Mon Nov  7 14:04:38 2022: Enabled event sources: k8s_audit, syscall
Mon Nov  7 14:04:38 2022: Opening capture with plugin 'k8saudit'
```

# Laat een audit regel voorbij komen
Zorg dat de logs in follow mode staan
```
kubectl -n default logs -l app.kubernetes.io/name=falco -f
```

Op mijn systeem komt het wat later aan als ik verwacht, maar uiteindelijk:
```
14:13:09.226874000: Warning K8s configmap with private credential (user=minikube-user verb=create resource=configmaps configmap=myconfigmap config={"password":"123456","username":"admin"})
```

Nog een Falco regel die we kunnen laten langskomen
```
kubectl exec $(kubectl get pods -l app.kubernetes.io/name=falco -o name) -- touch /bin/test-bin
```

Met als log melding
```
14:17:05.152497885: Error File below a known binary directory opened for writing (user=<NA> user_loginuid=-1 command=touch /bin/test-bin pid=22664 file=/bin/test-bin parent=<NA> pcmdline=<NA> gparent=<NA> container_id=44a59e8722f9 image=falcosecurity/falco-no-driver) k8s.ns=default k8s.pod=falco-srzxn container=44a59e8722f9
```

# Een kwaadwillende pod
Een kwaadwillende pod deployen om te onderzoeken met Falco.
Eerst een docker container maken die kwaadwillend is en daarna uploaden naar docker.io.

Dockerfile maken om een container te compileren
```
cd  ~/workshop-46-2022/Dockerfile
```

```
cat << EOF > ~/workshop-46-2022/Dockerfile
FROM bash:latest
COPY script.sh /
CMD ["bash", "/script.sh"]
EOF
```

Een script maken met acties die Falco zal herkennen
```
cat << EOF > ~/workshop-46-2022/script.sh
while true
  do
    mkdir /etc
    touch /etc/shadow
    cat /etc/shadow
    echo 'eviluser:$6$LRRTDsnZj$D97KEVPv51Kh0AG41iSK2J.KE9u83sAzKEEZJjmpWFjqouFfYchTQgnM6oBoqO/SJT6Bn4CXYqGkJXZbAg21P1:18975:0:99999:7:::' >> /etc/shadow
    sleep 30
done
sleep 3600
EOF
```

Daarna container bouwen en naar je docker repo uploaden
```
docker build -f ~/workshop-46-2022/Dockerfile . -t evilpod:0.0.1
docker images | grep evilpod
docker login
docker tag evilpod:0.0.1 bloemkool73/evilpod:latest
docker push bloemkool73/evilpod:latest
```

Dan pod deployen met de slechte code hierin.
```
cat << EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: evilpod
spec:
  containers:
  - name: evilpod
    image: bloemkool73/evilpod:latest
EOF
```

Bekijk nu de logs van Falco.

# Configuratie van Falco veranderen
We willen niet dat Falco de veranderingen aan de /etc/shadow file opmerkt.
Dus moeten we de config veranderen van Falco. Dit is wat lastiger omdat die
default config in de container zit.

# De hele boel opruimen
```
minikube stop
minikube delete --purge
rm -rf ~/workshop-46-2022
```
